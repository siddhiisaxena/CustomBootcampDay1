{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Spark**\n",
        "\n",
        "* If Data contains velocity, variety, volume & veracity.\n",
        "* Hadoop only works on **'On Disk'** computation and **Batch Data**. It has lengthy and complex framework. Low Cost\n",
        "* Hadoop has two main components -\n",
        "    * **HDFS** - Stores data in distributed fashion. Scaling is easier here.\n",
        "    * **Map Reduce** - used for distributed processing.\n",
        "* If you want to run SQL on hadoop then you need to learn HIVE\n",
        "* Hbase, Apache Storm (Handling realtime data), oozie, Scoop, pig.\n",
        "* Spark supports both realtime and batch processing. High Cost\n",
        "* In memory computation is supported i.e transformations are done on RAM, read write happens on disk. Supports tools like Spark SWL, Mlib, GraphX, and Spark Streaming.\n",
        "* Spark is simple and user friendly system.\n",
        "* If you want to do 10 different things you need to operate 10 different tools, to overcome this Spark was introduced. It is 100x faster than Hadoop. This is made possible by reducing the number of read/write iperations on the disk.\n",
        "* 350+ projects are there under Apache Foundation, Spark is one of them.\n",
        "* Spark can be used with Java, Scala, Python, SQL, and R.\n",
        "* To run Spark, Databricks was introduced.\n",
        "* Microsoft Fabric, Azure Synapse, AWS Blue.\n",
        "* Databricks is preffered to run Spark. (Why??)\n",
        "\n",
        "\n",
        "Apache Spark unifies\n",
        "  * Batch Processing\n",
        "  * Stream Analytics\n",
        "  * Machine Learning\n",
        "  * SQL Processing\n",
        "\n",
        "\n",
        "\n",
        "#### **Spark's Basic Architecture**\n",
        "------\n",
        "![Alt text](https://hacked.work/blog/wp-content/uploads/2015/03/spark-cluster.png)\n",
        "\n",
        "\n",
        "\n",
        "***Apache Spark works in a master-slave architecture where the master is called “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context (Spark Context) that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager.***\n",
        "\n",
        "\n",
        "* **Driver Program** – The process running the main() function of the application and creating the SparkContext. It is also the program/job, written by the developers which is submitted to Spark for processing. Driver program will partition the data. There will always be only 1 driver program.\n",
        "\n",
        "\n",
        "* **Spark Context** – Spark Context is the entry point to use Spark Core services and features. It sets up internal services and establishes a connection to a Spark execution environment. It communicates with cluster and to create RDD. Every Spark job creates a spark context object before it can do any processing. It allows your Spark Application to access Spark Context with the help of resource manager. It will start the Driver Program. ***There is one Spark Context per JVM***\n",
        "\n",
        "\n",
        "\n",
        "* **Cluster Manager** – Spark uses cluster manager to acquire resources across the cluster for executing a job. However, Spark is also agnostic of cluster managers and does not really care how it can get its hands on cluster resources. It supports the following cluster managers\n",
        "\n",
        "    * Spark standalone cluster manager - A simple cluster manager included with spark that makes it easy to set up a cluster\n",
        "    * YARN - resource manager in hadoop2\n",
        "    * Mesos\n",
        "    * Kubernetes\n",
        "\n",
        "\n",
        "* **Worker Node** – Worker Nodes are nodes which actually do data processing/heavy lifting on data.\n",
        "\n",
        "* **Executor** – Executors are independent processes which run inside the Worker Nodes in their own JVMs. Data processing is actually done by these executor processes.\n",
        "\n",
        "\n",
        "* **Cache** – Data stored in physical memory. Jobs can cache data so that it does not need to re-compute RDDs and hence increases the performance storing intermediary data.\n",
        "\n",
        "\n",
        "* **Task** – A task is a unit of work performed independently by the executor on one partition.\n",
        "\n",
        "\n",
        "* **Partition** – Spark manages its data by splitting data into manageable chunks across the nodes in a cluster. These chunks are called partitions. The splitting of data is done in a way so that it leads to reduction of network traffic and also optimise the operations to be performed on the data.\n",
        "\n",
        "[Imp Link](https://www.mrstonewallin.com/post/spark-knowledge-series-i)\n",
        "\n",
        "\n",
        "\n",
        "#### **Spark Deployment Modes: Client Mode vs Cluster Mode**\n",
        "---\n",
        "<img src=\"https://th.bing.com/th/id/R.5b4223cfa8490f2a8ac960b3e3d3738b?rik=sN29WUb1k7JWxw&riu=http%3a%2f%2fblog.brainlounge.de%2fmemoryleaks%2f2018-12-getting-started-with-spark-on-kubernetes-deploy-modes.png&ehk=zTXeqqjcdNpkjexQ77%2bl3JSIvFN1ljY4scGGGNdGo6Y%3d&risl=&pid=ImgRaw&r=0\" width=\"550\" height=\"300\" />\n",
        "\n",
        "* **Cluster Mode:** In cluster mode, the driver runs on one of the worker nodes, and this node shows as a driver on the Spark Web UI of your application. cluster mode is used to run production jobs.\n",
        "* **Client Mode:** In client mode, the driver runs locally from where you are submitting your application using spark-submit command. client mode is majorly used for interactive and debugging purposes. Note that in client mode only the driver runs locally and all tasks run on cluster worker nodes.\n",
        "\n",
        "\n",
        "#### **Spark Toolset**\n",
        "\n",
        "-------\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1104/1*_Dy9w0lUXIeH6WHALkQC-g.png\" width=\"400\" height=\"400\" />\n",
        "\n",
        "#### **Data Structures in Spark: RDD, DataFrame, Dataset**\n",
        "------\n",
        "\n",
        "* **Resilient Distributed Dataset**: resilient, immutable, collection of data.\n",
        "  * **Resilient:** RDDs are fault tolerant\n",
        "  * **Collection of Data:** RDD holds data and appears to be scala collection.\n",
        "  * **Partition:** Sparks break RDD into smaller cgunks of data.\n",
        "  * **Distributed:** Spark distributes the partition along the cluster.\n",
        "\\n\n",
        "\n",
        "* **Dataframe:** Most common Structured API and simply represents a table of data with rows and columns. Similar to DB table. The list that defines the columns and the types within those columns is called Schema.\n",
        "\n",
        "\n",
        "* **Dataset:**\n",
        "\n",
        "\n",
        "#### **Notes:**\n",
        "---\n",
        "\n",
        "* All Spark is lazily evaulated.\n",
        "* Spark context only creates rdd, dataframe.\n",
        "* **Spark Session:** can be used to create rdd, dataframe, dataset."
      ],
      "metadata": {
        "id": "LHCPlCmn1Bvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hands On**"
      ],
      "metadata": {
        "id": "9AP85dZbdq1p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfzEstnF03sL",
        "outputId": "3e48218d-ed3e-4c4f-c767-29dc799d3c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=0af94a29d71cfa1d3eed9c7aa58655591675f8cff5856b1eae38253ed73aa864\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "pyspark.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xW1Vg9Mk4pux",
        "outputId": "aa469e9d-b73d-4f47-f5c5-71903a0db557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "# import findspark\n",
        "from pyspark import SparkContext\n",
        "pyspark.__version__"
      ],
      "metadata": {
        "id": "wq5p_pkz_D0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7583cafc-af97-4bff-e0de-4a163ad53815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark.init('usr/local/spark')"
      ],
      "metadata": {
        "id": "EyjIv_fyb83y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf=pyspark.SparkConf().setMaster('local').setAppName(\"first\")\n",
        "# Creating a Spark Context\n",
        "sc = SparkContext(conf = conf)"
      ],
      "metadata": {
        "id": "OyIpBaGBb_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating rdd on Spark Context\n",
        "rdd = sc.parallelize([1,2,3]) #Creating simple array type object"
      ],
      "metadata": {
        "id": "2o3qyGRJdYrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDWYihN0dcdC",
        "outputId": "a268616b-c01c-49ce-ce76-adeebd8fc4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "9HdTLiWdddcY",
        "outputId": "26daaf49-c022-40fe-d52f-4a29020ec2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local appName=first>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a0e9cdae5b77:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>first</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = sc.parallelize(['Python','SQL','PySpark'])\n",
        "rdd2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3CpTbZx1Hvs",
        "outputId": "84644174-d5f8-4367-f5c4-7011775bbd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Python', 'SQL', 'PySpark']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(rdd2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YxGmnwK1H26",
        "outputId": "4ff46d7e-159f-427e-ff35-7a540d0ab718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
        "rdd3.collect()\n",
        "type(rdd3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti4X-3NAhFcI",
        "outputId": "298c3f6b-bb3a-41b0-c81f-3a6010c94015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4 = rdd3.map(lambda x: x*2)"
      ],
      "metadata": {
        "id": "MWeyYmO508wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd4.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c6vdehQ0_x8",
        "outputId": "cbdfdeb8-0085-4de2-ce1c-b0a67765afa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8, 10, 12, 14, 16, 18]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd5 = rdd3.filter(lambda x:x%2==0)"
      ],
      "metadata": {
        "id": "jMTRW_ho1CSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd5.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uUW3s5m1c_g",
        "outputId": "7b29b263-4683-400e-e64a-af2a2697bb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 6, 8]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [1, 2, 3, 4, 5, 6]\n",
        "even_sum = 0\n",
        "for num in numbers:\n",
        "  if num % 2 == 0:\n",
        "    even_sum += num\n",
        "\n",
        "print(even_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIfcXi-71evL",
        "outputId": "b26440eb-bf9f-4dc9-9600-1383e76a786f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6]\n",
        "even_sum = sum([x for x in numbers if x % 2 == 0])\n",
        "print(even_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ejg79EV8Ii",
        "outputId": "362f1db9-1617-4916-9601-2c58fa393957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [1, 2, 3, 4, 5, 6]\n",
        "even_sum = 0\n",
        "for i in range(len(numbers)):\n",
        "  if numbers[i] % 2 == 0:\n",
        "    even_sum += numbers[i]\n",
        "\n",
        "print(even_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZSWZFkWWD2m",
        "outputId": "f35e0a13-d490-47df-dd8f-8d96df501959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [1, 2, 3, 4, 5, 6]\n",
        "even_sum = 0\n",
        "while len(numbers) > 0:\n",
        "  num = numbers.pop()\n",
        "  if num % 2 == 0:\n",
        "    even_sum += num\n",
        "\n",
        "print(even_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3EDPKtsWOQS",
        "outputId": "8f212be2-7aaa-42e0-9882-19b6d669abfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B2UHiSZwWT7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}